{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: click in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (2022.3.15)\n",
      "Requirement already satisfied: joblib in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/yohanismael/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yohanismael/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yohanismael/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/yohanismael/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/yohanismael/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/yohanismael/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "Finished.\n",
      "Requirement already satisfied: nltk in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: joblib in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: click in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from nltk) (8.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gensim in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (4.3.0)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from gensim) (5.1.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.23.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.9.3)\n",
      "Requirement already satisfied: pyfume in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
      "Requirement already satisfied: pandas in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.6)\n",
      "Requirement already satisfied: six>=1.5 in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: fst-pso in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: simpful in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.9.0)\n",
      "Requirement already satisfied: miniful in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "Requirement already satisfied: requests in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/yohanismael/opt/anaconda3/lib/python3.9/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim) (1.26.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U textblob\n",
    "!python -m textblob.download_corpora\n",
    "\n",
    "%pip install nltk\n",
    "\n",
    "%pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_timestamp(s):\n",
    "    out=[]\n",
    "\n",
    "    s2=str.split(s,'+')\n",
    "    s3=str.split(s2[0],' ')\n",
    "\n",
    "    s4=str.split(s3[0],'-')\n",
    "\n",
    "    s5=str.split(s3[1],':')\n",
    "    year=int(s4[0])\n",
    "    month=int(s4[1])\n",
    "    day=int(s4[2])\n",
    "    hour=int(s5[0])\n",
    "    minute=int(s5[1])\n",
    "    second=int(s5[2])\n",
    "\n",
    "    return(datetime(year=year,month=month,day=day,hour=hour,minute=minute,second=second).timestamp())\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def NumStr(mot):\n",
    "    res = ''.join(filter(lambda i: i.isdigit(), mot))\n",
    "    if (res==''):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def extract_keywords(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Apply part-of-speech tagging\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Select only nouns and adjectives\n",
    "    keywords = [word for word, pos in tagged_tokens if (pos in ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS'] and not NumStr(word))]\n",
    "\n",
    "    return keywords\n",
    "\n",
    "def extract_sentiment_polarity(text):\n",
    "    blob=TextBlob(text)\n",
    "    return(blob.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end of keywords\n",
      "end of polarity\n",
      "end of timestamps\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('data/minnesota.csv')\n",
    "df=data\n",
    "print('start')\n",
    "\n",
    "df['keywords'] = df['tweet_text'].apply(extract_keywords)\n",
    "\n",
    "print('end of keywords')\n",
    "\n",
    "df['sentiment_polarity']=df['tweet_text'].apply(extract_sentiment_polarity)\n",
    "\n",
    "print('end of polarity')\n",
    "\n",
    "df['timestamps']=df['tweet_created_dt'].apply(str_to_timestamp)\n",
    "df['timestamps']=df['timestamps']-df['timestamps'].min()\n",
    "\n",
    "print('end of timestamps')\n",
    "\n",
    "clean_data=df\n",
    "clean_data.to_csv(\"data/clean_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(df,size,sliding):\n",
    "    X_out=[]\n",
    "\n",
    "    for time in range(0,int(df['timestamps'].max()),sliding):\n",
    "        o=df[df.timestamps<time+size]\n",
    "        X_out.append(o[o.timestamps>time])\n",
    "\n",
    "        print(\"%.3f\" % (time/int(df['timestamps'].max())), end=\"\\r\")\n",
    "    print(\"finish\")\n",
    "    return X_out\n",
    "\n",
    "def dfToListofVectors(df):\n",
    "    out=[]\n",
    "    for l_vect in df.vectors:\n",
    "        for v in l_vect:\n",
    "            \n",
    "            out.append(v)\n",
    "            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "time_day=60*60*24\n",
    "L=sliding_window(clean_data,time_day*4,time_day)\n",
    "first=L[61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=first['keywords'], vector_size=100, window=0, min_count=0, workers=0)\n",
    "model.save(\"Word2vec_models/test_word2vec.model\")\n",
    "word_vectors = model.wv\n",
    "del model\n",
    "\n",
    "def word2vectors(l):\n",
    "    try:\n",
    "        x = word_vectors[l]\n",
    "    except KeyError:\n",
    "        return \n",
    "    return x\n",
    "    \n",
    "\n",
    "def list2vectors(L):\n",
    "    out=[]\n",
    "    for i in L:\n",
    "        out.append(word2vectors(i))\n",
    "    return(np.asarray(out,dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for keywords in first.keywords:\n",
    "    for keyword in keywords:\n",
    "        i+=1\n",
    "\n",
    "arr=np.ndarray((i,100))\n",
    "i=0\n",
    "for keywords in first.keywords:\n",
    "\n",
    "    for keyword in keywords:\n",
    "        \n",
    "        arr[i]=word_vectors[keyword]\n",
    "        i+=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import k_means\n",
    "X=arr\n",
    "kmeans=k_means(X,n_clusters=5,n_init='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids=kmeans[0]\n",
    "centroids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('blacklivesmatter', 1.0000001192092896)]\n",
      "[('georgefloyd', 0.5381887555122375)]\n",
      "[('black', 0.6451596617698669)]\n",
      "[('floyd', 0.9999999403953552)]\n",
      "[('matter', 0.5804711580276489)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(centroids.shape[0]):\n",
    "    print(word_vectors.similar_by_vector(centroids[i],topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7339996d7815a0ae30850c182ba9bbc0a1cb89e61563a0ae1819bddf17b67295"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
